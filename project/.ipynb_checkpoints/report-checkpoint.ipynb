{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preliminary Note: Recall that I cannot use any specifically markdown formatting in the notebook, since opening it in PyCharm (a necessary operation to ensure it has saved code appropriately) will permanently delete any cell with special markdown formatting.\n",
    "\n",
    "Vision Revision:\n",
    "For my final project, I propose to build a neural network trained to generate poetry in the style of John  Milton. My dataset will be a simple text file containing the entirety of Paradise Lost (about 11,000 lines of unrhyming iambic pentameter (also known as blank verse)). My hope for this project is that it can learn how to generate real words (or at least words that sound real) as well as poetry with the right number of syllables per line.\n",
    "\n",
    "Background:\n",
    "This project will use an LSTM (Long short-term memory) trained on the text of Paradise Lost. An LSTM is a specific type of recurrent neural network (RNN) which is structured in order to be able to learn longer term interdependencies between inputs. This LSTM will analyze the text of Paradise Lost, tokenized by letter, and will generate output coorespondingly. My model for this will be Chollet (https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb) with some cross-referencing of another example (https://github.com/pranjal52/text_generators) which was unfortunately running too slow on the lab computers to be very useful, but did provide some structural guidance. While it might seem intuitively obvious to tokenize this sort of data at the level of words, or possibly syllables with some additional background work, almost every single useful example I could find of textual generation with an LSTM tokenizes at the level of individual characters (letters and punctuation). For the purposes of this project, I have assumed that there is a legitimate reason for this, althouh I am not sure what that reason is. My best guess is that, for datasets that would take a shorter amount of time than a week to run (or datasets of a fixed, specifically small size, such as this one), there just isn't enough word data to tokenize at a level of resolution that high. In otherwords, tokenizing at the level of individual letters rather than words effectively (at least) triples the size of your dataset for learning, and still produces somewhat interesting results. Other examples (than the above two) who tokenize by letter are listed below.\n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "https://medium.freecodecamp.org/applied-introduction-to-lstms-for-text-generation-380158b29fb3\n",
    "https://towardsdatascience.com/generating-text-using-a-recurrent-neural-network-1c3bfee27a5e\n",
    "\n",
    "Implementation:\n",
    "My implementation is primarily based on Chollet's, linked above. This implementation extracts sequences of a given character length from the text. These sequences overlap partially, are one-hot encoded, and get stored neatly in an array. We then prepare an array of target characters that come after each sequence.\n",
    "The LSTM itself uses a single, medium layer of nodes to learn these sequences and which characters typically follow which other characters. It then has a dense, softmax layer to provide us with character probabilities.\n",
    "To generate text, we take a seed of a specific length at random from the text, draw a probability distribution over the next character according to the current sequence, and randomly choose according to that distribution what the next character will be. We use a \"temperature\" variable to control how easy or difficult it is for the randomly chosen character to deviate from the most likely character. We generate text at several different temperatures in order to see which level of variation is the most like real text.\n",
    "\n",
    "Results:\n",
    "Before we get to the results proper, there are a few oddities worth noting about the LSTM. And these oddities are results themselves of this project, even if they are more meta-observations that I have gathered through my tinkering with the LSTM than strict results. Firstly, the LSTM only uses one layer, and additional layers do not seem to improve the performance noticeably, while they do drastically increase the training time. In addition, training the model for multiple epochs comes at no noticeable benefit to offset the increased training time. It is not perfectly clear to me why either of these are the case. That being said, my guess is that the weights in the LSTM get affected by the vanishing gradient problem, and so do not get adjusted with the additional training of more epochs, and do not benefit from the depth of more layers.\n",
    "As for the results proper, they are promising! The generated text is somewhat believable, at least at lower temperatures, and I was able to almost learn the line-breaks appropriate to iambic pentameter. It is supposed to have 10 syllables per line (and a particular pattern of stressed/unstressed syllables, but that is too subtle for this network, I think), and it is usually a few syllables short or long, but in the right range. In addition, it has learned some of Milton's stylistic, poetic quirks, such as saving a syllable by dropping vowels at the end of words, as in \"heav'n\" and \"seem'd.\" One particular oddity of my LSTM is its affinity for beginnin words with \"s\" and its overuse of \"the\" and \"and.\" While of course those are very common words, it uses them even more often than I would expect of real text. And Milton does not seem to begin a disproportionate amount of words with \"s,\" so both of these quirks have stymied me to some degree. As a final observation, it is rather interesting to observe that, at higher temperatures, the text that is generated seems very much like a language, just one that you don't know. If you read the highest temperature out loud, for example, it often sounds sort of like hearing someone read The Canterbury Tales, in Middle English, untranslated. (For an example of what I mean, see: https://www.youtube.com/watch?v=GihrWuysnrc). It is for that reason that I consider this project a something of a success. Even if the poetry it generates is not very believably Miltonic, it does seem to have picked up on many of the subtleties of intra-word-structure such that it can generate text that sounds like a language but is definitely not.\n",
    "\n",
    "Implications:\n",
    "Poetry is weird. A conglomeration of semi-coherent, half-connected words and phrases whose structure and interplay somehow amount to far more than their mere parts -- it does not seem like the sort of thing that a NN or AI (at least currently) could convincingly fake. It's too human, too subjective, and toes the line between nonsense and beauty far too closely. But the fact that it is subjective and often so open to interpretation is exactly what makes the potential for NN-generated poetry so great. If someone doesn't know that a poem has been generated by an un-thinking AI (and no AI that I am aware of can currently be described as thinking, in a human sense), they will impute their own interpretations and understandings into the poem and attribute them to the author. In other words, if a neural network can generate convingingly human poetry, it is due in large part to how nebulous and varying poetry is in the first place. But that fact is simultaneously why it would be difficult to train a neural network to generate convincing poetry. The variation and inconsistency that makes poetry such a uniquely human affair also makes it difficult to train a neural network on. This is one of the reasons why I have trouble believing any AI will ever be up to or exceed the capacity of a human beings in their own domains (obviously computers can easily surpass human beings at, say, raw computational power and things of that sort). There is something about us that makes us and our cultural activities distinct from, and greater than, mere mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 4608976\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "#text = open(\"projectData.txt\").read().lower()\n",
    "\n",
    "# I tested the generator on other text than Milton, too. alternativeData.txt currently \n",
    "# contains several of Charles Dickin's novels, amounting to a dataset about 10x the size of Milton's.\n",
    "text = open(\"alternativeData.txt\").read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 1536316\n",
      "Unique characters: 57\n",
      "['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 30\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "print(chars)\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "Epoch 1/1\n",
      "1536316/1536316 [==============================] - 520s 339us/step - loss: 1.7547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d3fa05828>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "print(\"epoch\")\n",
    "# Fit the model for 1 epochs on the available training data\n",
    "model.fit(x, y, batch_size=32, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \"mutanhed.'\n",
      "\n",
      "'you don't say so?\"\n",
      "------ temperature: 0.2\n",
      "mutanhed.'\n",
      "\n",
      "'you don't say so?' said the conting the state of the contented the looker and and and the stood the conting the stoom the do the conting the still the conting the bay, and the looker the stood the fing and the little the looker the strought the stortion and horse streat the conting the companion and the pressing the state of the conting the contention which he was the man and the poseron the still and the contenti\n",
      "------ temperature: 0.5\n",
      "ron the still and the contenting in\n",
      "the state the devious starterter restand his desrong a man at a the still and position indeid of the the wathrase of looked which he was it and see the companal the be mind she presented to be had the mands in the low state with considenture with it is be the not from a desenting or the who short the door that the det the great bell, and the person the ghad\n",
      "to the dression and little starter\n",
      "------ temperature: 1.0\n",
      "he dression and little startery.  rightd\n",
      "in the\n",
      "affases.\n",
      "\n",
      "'i wimmy.   xleated ansom in contence do it, but poandleed, he never moment at do\n",
      "mr. dinges with the called clads no, pulcoinsy when he sa\n",
      "counting it, sleep, reeld, the dire's bays bacy do his poet.  i', who said his mannfaring horseited race bay, afver but they theered sgean degs insigry, 'whaud't's yes, att drowing bread\n",
      "\n",
      "pited pessing i k"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aqv2/Desktop/cs344-masterfolder/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowd or indeightaed peftars\n",
      "------ temperature: 1.2\n",
      "i knowd or indeightaed peftarse thinkos,' replied shr. nith-take feel good\n",
      "tatfence at a\n",
      "mr, he,' said the thecrendory\n",
      "it it rhclishing glad tas flowirtiening to srelsapeced by brave consted arrlole videny putty sleepder, and the plailer from great\n",
      "spokpaectle withdo upon it, this brought ainding thought of his has, and streaks a\n",
      "right, was theme crofetsiduwionar contin 'consressing as\n",
      "bhoged a watnmome, stocriting ate is spun\n"
     ]
    }
   ],
   "source": [
    "# Select a text seed at random\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "\n",
    "    # We generate 400 characters\n",
    "    for i in range(400):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
